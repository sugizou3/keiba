{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import tqdm\n",
    "from torch import nn\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "torch.set_printoptions(edgeitems=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pickle/pickle_r_all\", \"rb\") as f:\n",
    "    r = pickle.load(f)\n",
    "with open(\"pickle/pickle_p_all\", \"rb\") as f:\n",
    "    p = pickle.load(f)\n",
    "# with open(\"pickle/pickle_st_all\", \"rb\") as f:\n",
    "#     st = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = r.data_c[r.data_c['date']>'2024-07-01']\n",
    "data = r.data_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Class.LearningDataProcessor import RacingDataProcessor\n",
    "from Class.Model import HorseRaceModel\n",
    "from Class.LearningModule import LearningModule\n",
    "from Func.Deeplearning_func import set_criterion\n",
    "from Func.Deeplearning_func import log_func\n",
    "from Func.Deeplearning_func import create_predictionTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary', '64', '14', '0.3', '32', '32']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sugiy\\AppData\\Local\\Temp\\ipykernel_53140\\1688076967.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net = torch.load('{}/{}/{}'.format(base_path,folder_name,file_name))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m folder_list:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_predictionTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36mcreate_predictionTable\u001b[1;34m(base_path, folder_name)\u001b[0m\n\u001b[0;32m     30\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m     31\u001b[0m lem \u001b[38;5;241m=\u001b[39m LearningModule(rdp,net,data,task_type,train_split,device,optimizer, criterion,log_func)\n\u001b[1;32m---> 33\u001b[0m mean_loss,df \u001b[38;5;241m=\u001b[39m \u001b[43mlem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[6], line 150\u001b[0m, in \u001b[0;36mLearningModule.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m         mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m mean_loss) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    149\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_pred_results(horse_id, pred, date)\n\u001b[1;32m--> 150\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pred_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m         df_list \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_list,df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_func(df_list,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type)\n",
      "Cell \u001b[1;32mIn[6], line 88\u001b[0m, in \u001b[0;36mLearningModule.process_pred_table\u001b[1;34m(self, results)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_pred_table\u001b[39m(\u001b[38;5;28mself\u001b[39m,results):\n\u001b[1;32m---> 88\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhorse_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:886\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1125\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# make mypy happy\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masof\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1759\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     _, lidx, ridx \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mjoin(right, how\u001b[38;5;241m=\u001b[39mhow, return_indexers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1759\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m \u001b[43mget_join_indexers_non_unique\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lidx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(lidx, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m   1764\u001b[0m     lidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1793\u001b[0m, in \u001b[0;36mget_join_indexers_non_unique\u001b[1;34m(left, right, sort, how)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_join_indexers_non_unique\u001b[39m(\n\u001b[0;32m   1771\u001b[0m     left: ArrayLike,\n\u001b[0;32m   1772\u001b[0m     right: ArrayLike,\n\u001b[0;32m   1773\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1774\u001b[0m     how: JoinHow \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m    Get join indexers for left and right.\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;124;03m        Indexer into right.\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1793\u001b[0m     lkey, rkey, count \u001b[38;5;241m=\u001b[39m \u001b[43m_factorize_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1795\u001b[0m         lidx, ridx \u001b[38;5;241m=\u001b[39m libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join(lkey, rkey, count, sort\u001b[38;5;241m=\u001b[39msort)\n",
      "File \u001b[1;32mc:\\Users\\sugiy\\anaconda3\\envs\\keiba\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2561\u001b[0m, in \u001b[0;36m_factorize_keys\u001b[1;34m(lk, rk, sort)\u001b[0m\n\u001b[0;32m   2554\u001b[0m     rlab \u001b[38;5;241m=\u001b[39m rizer\u001b[38;5;241m.\u001b[39mfactorize(\n\u001b[0;32m   2555\u001b[0m         rk\u001b[38;5;241m.\u001b[39mto_numpy(na_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mlk\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnumpy_dtype), mask\u001b[38;5;241m=\u001b[39mrk\u001b[38;5;241m.\u001b[39misna()\n\u001b[0;32m   2556\u001b[0m     )\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2558\u001b[0m     \u001b[38;5;66;03m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;66;03m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;66;03m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[1;32m-> 2561\u001b[0m     llab \u001b[38;5;241m=\u001b[39m \u001b[43mrizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2562\u001b[0m     rlab \u001b[38;5;241m=\u001b[39m rizer\u001b[38;5;241m.\u001b[39mfactorize(rk)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m llab\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mintp), llab\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"model\"\n",
    "folder_list = os.listdir(base_path)\n",
    "folder_list = [0:5]\n",
    "df_list = data\n",
    "\n",
    "results = []\n",
    "for folder in folder_list:\n",
    "    try:\n",
    "        df = create_predictionTable(base_path, folder)\n",
    "        results.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing {folder}: {e}\")\n",
    "\n",
    "# 各データフレームを結合する\n",
    "dfs = [df.rename(columns={'pred': f'pred_{i}', 'rank': f'rank_{i}'}) for i, df in enumerate(results)]\n",
    "df_list_combined = pd.concat([df_list] + dfs, axis=1).merge(df_list[['index', 'horse_id']], on=['horse_id', 'index'], how='left')\n",
    "\n",
    "df_list.to_pickle('pickle/unsambleList.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary', '64', '14', '0.3', '32', '32']\n",
      "['binary', '64', '18', '0.3', '32', '32']\n",
      "['list_net', '16', '10', '2', '16', '16']\n",
      "['list_net', '16', '10', '2', '32', '32']\n",
      "['list_net', '32', '10', '2', '16', '16']\n",
      "['list_net', '32', '10', '2', '32', '32']\n",
      "['list_net', '64', '10', '2', '16', '16']\n",
      "['list_net', '64', '10', '2', '32', '32']\n",
      "['list_net', '64', '18', '2', '32', '32']\n",
      "['regression', '128', '10', '0.3', '16', '16']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sugiy\\AppData\\Local\\Temp\\ipykernel_14804\\1688076967.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net = torch.load('{}/{}/{}'.format(base_path,folder_name,file_name))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "# ベースパスとフォルダリストの初期設定\n",
    "base_path = \"model\"\n",
    "folder_list = os.listdir(base_path)  # これはデバッグ用に最初の3つのフォルダに限定しています\n",
    "df_list = data\n",
    "\n",
    "# 任意のスレッド数を指定\n",
    "num_threads = 10  # 任意のスレッド数を指定\n",
    "\n",
    "# フォルダを処理する関数\n",
    "def process_folder(folder, idx):\n",
    "    try:\n",
    "        return create_predictionTable(base_path, folder).rename(\n",
    "            columns={'pred': f'pred_{idx}', 'rank': f'rank_{idx}'}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {folder}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 並列処理の実行\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    results = list(executor.map(lambda idx_folder: process_folder(idx_folder[1], idx_folder[0]), enumerate(folder_list)))\n",
    "\n",
    "# None を取り除いて df_list にマージ\n",
    "for i, df in enumerate(results):\n",
    "    if df is not None:  # Noneチェックを追加\n",
    "        df_list = df_list.merge(df[[f'pred_{i}', f'rank_{i}', 'date', 'horse_id']], on=['date', 'horse_id'], how='left')\n",
    "\n",
    "# ピクルファイルに保存\n",
    "df_list.to_pickle('pickle/unsambleList.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keiba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
