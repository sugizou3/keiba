{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue = {'01':'札幌','02':'函館','03':'福島','04':'新潟','05':'東京','06':'中山','07':'中京','08':'京都','09':'阪神','10':'小倉'}\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from urllib.request import urlopen\n",
    "from Class.RaceFeature import RaceFeature\n",
    "from Class.Results import Results\n",
    "from Class.Peds import Peds\n",
    "from Class.ShutubaTable import ShutubaTable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "# pd.set_option('display.max_columns', 200)\n",
    "# np.set_printoptions(threshold=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from  urllib.request import urlopen\n",
    "\n",
    "class Return:\n",
    "    def __init__(self,return_tables):\n",
    "        self.return_tables = return_tables\n",
    "        # self.hukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls,path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list]) \n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list,pre_df):\n",
    "        return_tables = {}\n",
    "        if pre_df is not None:\n",
    "            pre_list=pre_df.index.unique()\n",
    "            race_id_list = [race_id  for race_id in race_id_list if not race_id in pre_list]\n",
    "        for race_id in tqdm.tqdm(race_id_list):\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/race/'+race_id\n",
    "                f = urlopen(url)\n",
    "                html = f.read()\n",
    "                html = html.replace(b'<br />', b'br')\n",
    "                html = urlopen(url).read()\n",
    "                dfs = pd.read_html(html)\n",
    "                return_tables[race_id]  = pd.concat([dfs[1],dfs[2]])\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "        \n",
    "        for key in return_tables.keys():\n",
    "            return_tables[key].index = [key]*len(return_tables[key])\n",
    "\n",
    "        return_tables = pd.concat([return_tables[key] for key in return_tables.keys()],sort=False)\n",
    "        df_return_tables = pd.DataFrame(return_tables)\n",
    "\n",
    "        df_return_tables = pd.concat([pre_df,df_return_tables],axis=0)\n",
    "        \n",
    "\n",
    "        return df_return_tables\n",
    "            \n",
    "    @property\n",
    "    def hukusho(self):\n",
    "        hukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "        wins = hukusho[1].str.split(' ',expand=True)[[0,1,2]]\n",
    "        wins.columns = ['win_0','win_1','win_2']\n",
    "        returns = hukusho[2].str.split(' ',expand=True)[[0,1,2]]\n",
    "        returns.columns = ['return_0','return_1','return_2']\n",
    "        df = pd.concat([wins,returns],axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',','')\n",
    "        return df.fillna(0).astype(int) \n",
    "\n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0]=='単勝'][[1,2]]\n",
    "        tansho.columns = ['win','return']\n",
    "\n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column],errors='coerce')\n",
    "\n",
    "        return tansho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_tables = {}\n",
    "# pre_df=None\n",
    "# race_id_list = r.data.index.unique()\n",
    "# race_id_list = race_id_list[:3]\n",
    "# if pre_df is not None:\n",
    "#     pre_list=pre_df.index.unique()\n",
    "#     race_id_list = [race_id  for race_id in race_id_list if not race_id in pre_list]\n",
    "# for race_id in tqdm.tqdm(race_id_list):\n",
    "#     try:\n",
    "#         url = 'https://db.netkeiba.com/race/'+race_id\n",
    "#         f = urlopen(url)\n",
    "#         html = f.read()\n",
    "#         html = html.replace(b'<br />', b'br')\n",
    "#         html = urlopen(url).read()\n",
    "#         dfs = pd.read_html(html)\n",
    "#         return_tables[race_id]  = pd.concat([dfs[1],dfs[2]])\n",
    "#         time.sleep(1)\n",
    "#     except IndexError:\n",
    "#         print('tetete')\n",
    "#         continue\n",
    "#     except Exception as e:\n",
    "#         print('tetete')\n",
    "#         print(e)\n",
    "#         break\n",
    "\n",
    "# for key in return_tables.keys():\n",
    "#     return_tables[key].index = [key]*len(return_tables[key])\n",
    "# print('tetete')\n",
    "# return_tables = pd.concat([return_tables[key] for key in return_tables.keys()],sort=False)\n",
    "# df_return_tables = pd.DataFrame(return_tables)\n",
    "\n",
    "# df_return_tables = pd.concat([pre_df,df_return_tables],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Results.read_pickle(['pickle/results.pickle'])\n",
    "p = Peds.read_pickle(['pickle/peds.pickle'])\n",
    "rt = Return.read_pickle(['pickle/return_tables.pickle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_return_info(data,rt):\n",
    "#     df1 = pd.merge(\n",
    "#         data.reset_index(),\n",
    "#         rt.tansho.reset_index().rename(columns={'win': '馬番'})[['index', 'return', '馬番']],\n",
    "#         on=['index', '馬番'],\n",
    "#         how='left'\n",
    "#     ).fillna(0)\n",
    "\n",
    "#     # 複勝データ (rt.hukusho) を一度に縦持ちに変換\n",
    "#     hukusho_merged = pd.wide_to_long(\n",
    "#         rt.hukusho.reset_index(),\n",
    "#         stubnames=['win', 'return'],\n",
    "#         i='index',\n",
    "#         j='num',\n",
    "#         sep='_'\n",
    "#     ).reset_index().rename(columns={'win': '馬番', 'return': 'hukusho_return'})\n",
    "\n",
    "#     # 単勝データと複勝データをマージし、複勝リターンを結合\n",
    "#     df1 = pd.merge(df1, hukusho_merged[['index', '馬番', 'hukusho_return']], on=['index', '馬番'], how='left')\n",
    "\n",
    "#     df1 = df1.fillna(0).set_index('index')\n",
    "#     df1 = df1.rename(columns={'return': 'tansho_return'})\n",
    "#     df1['tansho_return'] = df1['tansho_return']*0.01\n",
    "#     df1['hukusho_return'] = df1['hukusho_return']*0.01\n",
    "#     return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "r.preprocessing()\n",
    "p.get_no_peds_list(r.data_p)\n",
    "p.encode()\n",
    "r.get_previous_result()\n",
    "# r.merge_peds(p.peds_e)\n",
    "r.data_pe = r.data_p\n",
    "r.process_categorical()\n",
    "r.get_return_info(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pickle/pickle_r_all\", \"wb\") as f:\n",
    "    pickle.dump(r, f)\n",
    "\n",
    "with open(\"pickle/pickle_p_all\", \"wb\") as f:\n",
    "    pickle.dump(p, f)\n",
    "\n",
    "\n",
    "# with open(\"pickle/pickle_r_all\", \"rb\") as f:\n",
    "#     r = pickle.load(f)\n",
    "# with open(\"pickle/pickle_p_all\", \"rb\") as f:\n",
    "#     p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shutuba_table = ShutubaTable.scrape_shutuba_table_ChromeDriver(['202404030810','202404030811'],'2024-05-01')\n",
    "# shutuba_table = pd.read_pickle('pickle/shutuba_table_sample.pickle')\n",
    "# st = ShutubaTable(shutuba_table)\n",
    "# st.data_p = shutuba_table\n",
    "# p.get_no_peds_list(st.data_p)\n",
    "# p.encode()\n",
    "# st.read_encode_data(r)\n",
    "# st.merge_peds(p.peds_e)\n",
    "# st.process_categorical()\n",
    "# st.data_c = pd.concat([st.data_c,r.data_c])\n",
    "# st.data_c = st.data_c.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"pickle/pickle_r_all\", \"wb\") as f:\n",
    "#     pickle.dump(r, f)\n",
    "\n",
    "# with open(\"pickle/pickle_p_all\", \"wb\") as f:\n",
    "#     pickle.dump(p, f)\n",
    "\n",
    "\n",
    "# with open(\"pickle/pickle_r_all\", \"rb\") as f:\n",
    "#     hoge = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_horse_list = st.data['horse_id'].unique()\n",
    "# for i in range(4):\n",
    "#     pred_index_list = r.data[r.data['horse_id'].isin(pred_horse_list)].index.unique()\n",
    "#     pred_horse_list = r.data[r.data.index.isin(pred_index_list)]['horse_id'].unique()\n",
    "# data =r.data_c[ r.data.index.isin(pred_index_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keiba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
